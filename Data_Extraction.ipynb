{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import mysql.connector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregarted_Insurance\n",
    "### Getting Data from /data/Aggregrated/Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/aggregated/insurance/country/india/state/' #1. Get path of particular folder where data exist\n",
    "agg_ins_list = os.listdir(path_1) # 2. List the directories in the path \n",
    "skl_data_1 = {'state' : [],'year' : [],'quarter': [],'transaction_type':[],'transaction_count' : [], 'transaction_amount': []} # 3. Create a empty dict to insert values that we get from data\n",
    "\n",
    "for state in agg_ins_list: # 4. Loop the process for below folders\n",
    "    curr_path = path_1  + state + '/'\n",
    "    year_list = os.listdir(curr_path)\n",
    "    #print(year_list)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_path + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        #print(file_list)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file) # ! Open and load json file\n",
    "            A = json.load(data)\n",
    "            \n",
    "            for item in A['data']['transactionData']:\n",
    "                name = item['name']\n",
    "                count = item['paymentInstruments'][0]['count']\n",
    "                amount = item['paymentInstruments'][0]['amount']\n",
    "                skl_data_1['state'].append(state) #6. Append it to the skeleton dataframe\n",
    "                skl_data_1['year'].append(year)\n",
    "                skl_data_1['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_1['transaction_type'].append(name)\n",
    "                skl_data_1['transaction_amount'].append(amount)\n",
    "                skl_data_1['transaction_count'].append(count)\n",
    "aggr_ins_df = pd.DataFrame(skl_data_1)\n",
    "aggr_ins_df['state'] = aggr_ins_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "aggr_ins_df['state'] = aggr_ins_df['state'].str.replace('-' ,' ') \n",
    "aggr_ins_df['state'] = aggr_ins_df['state'].str.title()\n",
    "aggr_ins_df['state'] = aggr_ins_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregarted_Transactions\n",
    "\n",
    "### Getting Data from /data/Aggregrated/Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2  =  \"/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/aggregated/transaction/country/india/state/\" # 1. Get path\n",
    "agg_trans_list = os.listdir(path_2) # 2. List the directories in the path \n",
    "#print(agg_state_list)\n",
    "skl_data_2 = {'state' : [],'year' : [],'quarter': [],'transaction_type':[],'transaction_count' : [], 'transaction_amount': []} # 3. Create a empty dict to insert values that we get from data\n",
    "\n",
    "for state in agg_trans_list: # 4. Loop the process for below folders\n",
    "    curr_path = path_2  + state + '/'\n",
    "    year_list = os.listdir(curr_path)\n",
    "    #print(year_list)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_path + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        #print(file_list)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file) # ! Open and load json file\n",
    "            B = json.load(data)\n",
    "            \n",
    "            for item in B['data']['transactionData']: # 5. From Json file select the requried data\n",
    "                name = item['name']\n",
    "                count = item['paymentInstruments'][0]['count']\n",
    "                amount = item['paymentInstruments'][0]['amount']\n",
    "                \n",
    "                skl_data_2['state'].append(state) #6. Append it to the skeleton dataframe\n",
    "                skl_data_2['year'].append(year)\n",
    "                skl_data_2['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_2['transaction_type'].append(name)\n",
    "                skl_data_2['transaction_amount'].append(amount)\n",
    "                skl_data_2['transaction_count'].append(count)\n",
    "                \n",
    "\n",
    "aggr_trans_df = pd.DataFrame(skl_data_2)\n",
    "aggr_trans_df['state'] = aggr_trans_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "aggr_trans_df['state'] = aggr_trans_df['state'].str.replace('-' ,' ') \n",
    "aggr_trans_df['state'] = aggr_trans_df['state'].str.title()\n",
    "aggr_trans_df['state'] = aggr_trans_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregarted_User\n",
    "\n",
    "### Getting Data from /data/Aggregrated/User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_3 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/aggregated/user/country/india/state/'\n",
    "agg_user_list = os.listdir(path_3)\n",
    "skl_data_3 = dict(state = [],year = [], quarter = [], brand = [],transaction_count = [],percentage = [] )\n",
    "\n",
    "\n",
    "for state in agg_user_list:\n",
    "    curr_state = path_3 + state + '/' \n",
    "    year_list = os.listdir(curr_state)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_state + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file)\n",
    "            C = json.load(data)\n",
    "            \n",
    "            try:\n",
    "                for item in C['data']['usersByDevice']:\n",
    "                    brand_name = item['brand']\n",
    "                    count = item['count']\n",
    "                    percentage = item['percentage']\n",
    "                    skl_data_3['state'].append(state)\n",
    "                    skl_data_3['year'].append(year)\n",
    "                    skl_data_3['quarter'].append(int(file.strip('.json')))\n",
    "                    skl_data_3['brand'].append(brand_name)\n",
    "                    skl_data_3['transaction_count'].append(count)\n",
    "                    skl_data_3['percentage'].append(percentage)\n",
    "            except Exception as err:\n",
    "                #print(err)\n",
    "                pass\n",
    "\n",
    "aggr_user_df = pd.DataFrame(skl_data_3)\n",
    "aggr_user_df['state'] = aggr_user_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "aggr_user_df['state'] = aggr_user_df['state'].str.replace('-' ,' ') \n",
    "aggr_user_df['state'] = aggr_user_df['state'].str.title()\n",
    "aggr_user_df['state'] = aggr_user_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    "\n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map_Insurance\n",
    "\n",
    "### Getting Data from /data/Map/Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_4 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/map/insurance/hover/country/india/state/'\n",
    "map_ins_list = os.listdir(path_4)\n",
    "skl_data_4 = dict(state = [],year = [], quarter = [], district = [],transaction_count = [],transaction_amount = [] )\n",
    "for state in map_ins_list:\n",
    "    curr_state = path_4 + state + '/' \n",
    "    year_list = os.listdir(curr_state)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_state + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file)\n",
    "            D = json.load(data)\n",
    "            \n",
    "            for item in D['data']['hoverDataList']:\n",
    "                distr_name = item['name']\n",
    "                count = item['metric'][0]['count']\n",
    "                amount = item['metric'][0]['amount']\n",
    "                skl_data_4['state'].append(state)\n",
    "                skl_data_4['year'].append(year)\n",
    "                skl_data_4['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_4['district'].append(distr_name)\n",
    "                skl_data_4['transaction_count'].append(count)\n",
    "                skl_data_4['transaction_amount'].append(amount)\n",
    "            \n",
    "map_ins_df = pd.DataFrame(skl_data_4)\n",
    "map_ins_df['state'] = map_ins_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "map_ins_df['state'] = map_ins_df['state'].str.replace('-' ,' ') \n",
    "map_ins_df['state'] = map_ins_df['state'].str.title()\n",
    "map_ins_df['state'] = map_ins_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    "\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map_Transaction\n",
    "\n",
    "### Getting Data from /data/Map/Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_5 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/map/transaction/hover/country/india/state/'\n",
    "map_trans_list = os.listdir(path_5)\n",
    "skl_data_5 = dict(state = [],year = [], quarter = [], district = [],transaction_count = [],transaction_amount = [] )\n",
    "for state in map_trans_list:\n",
    "    curr_state = path_5 + state + '/' \n",
    "    year_list = os.listdir(curr_state)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_state + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file)\n",
    "            E = json.load(data)\n",
    "            \n",
    "            for item in E['data']['hoverDataList']:\n",
    "                distr_name = item['name']\n",
    "                count = item['metric'][0]['count']\n",
    "                amount = item['metric'][0]['amount']\n",
    "                skl_data_5['state'].append(state)\n",
    "                skl_data_5['year'].append(year)\n",
    "                skl_data_5['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_5['district'].append(distr_name)\n",
    "                skl_data_5['transaction_count'].append(count)\n",
    "                skl_data_5['transaction_amount'].append(amount)\n",
    "            \n",
    "map_trans_df = pd.DataFrame(skl_data_5)\n",
    "map_trans_df['state'] = map_trans_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "map_trans_df['state'] = map_trans_df['state'].str.replace('-' ,' ') \n",
    "map_trans_df['state'] = map_trans_df['state'].str.title()\n",
    "map_trans_df['state'] = map_trans_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    "          \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map_User\n",
    "\n",
    "### Getting Data from /data/Map/User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_6 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/map/user/hover/country/india/state/'\n",
    "map_user_list = os.listdir(path_6)\n",
    "skl_data_6 = dict(state = [],year = [], quarter = [], district = [], registered_users  = [],app_opens = [] )\n",
    "for state in map_user_list:\n",
    "    curr_state = path_6 + state + '/' \n",
    "    year_list = os.listdir(curr_state)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_state + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file)\n",
    "            F = json.load(data)\n",
    "            \n",
    "            for item in F['data']['hoverData'].items():\n",
    "                distr_name = item[0] \n",
    "                reg_user_count = item[1]['registeredUsers']         \n",
    "                app_opens = item[1]['appOpens']\n",
    "                skl_data_6['state'].append(state)\n",
    "                skl_data_6['year'].append(year)\n",
    "                skl_data_6['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_6['district'].append(distr_name)\n",
    "                skl_data_6['registered_users'].append(reg_user_count)\n",
    "                skl_data_6['app_opens'].append(app_opens)\n",
    "\n",
    "map_user_df = pd.DataFrame(skl_data_6)\n",
    "map_user_df['state'] = map_user_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "map_user_df['state'] = map_user_df['state'].str.replace('-' ,' ') \n",
    "map_user_df['state'] = map_user_df['state'].str.title()\n",
    "map_user_df['state'] = map_user_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    "\n",
    " \n",
    "                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>district</th>\n",
       "      <th>registered_users</th>\n",
       "      <th>app_opens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>north and middle andaman district</td>\n",
       "      <td>10720</td>\n",
       "      <td>904869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>south andaman district</td>\n",
       "      <td>63487</td>\n",
       "      <td>2368756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>nicobars district</td>\n",
       "      <td>2081</td>\n",
       "      <td>573782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>north and middle andaman district</td>\n",
       "      <td>11379</td>\n",
       "      <td>963824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>south andaman district</td>\n",
       "      <td>66959</td>\n",
       "      <td>3404740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17563</th>\n",
       "      <td>Nagaland</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>tuensang district</td>\n",
       "      <td>1261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17564</th>\n",
       "      <td>Nagaland</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>peren district</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17565</th>\n",
       "      <td>Nagaland</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>dimapur district</td>\n",
       "      <td>22603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17566</th>\n",
       "      <td>Nagaland</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>zunheboto district</td>\n",
       "      <td>1099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17567</th>\n",
       "      <td>Nagaland</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>wokha district</td>\n",
       "      <td>1391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17568 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           state  year  quarter  \\\n",
       "0      Andaman & Nicobar Islands  2022        1   \n",
       "1      Andaman & Nicobar Islands  2022        1   \n",
       "2      Andaman & Nicobar Islands  2022        1   \n",
       "3      Andaman & Nicobar Islands  2022        2   \n",
       "4      Andaman & Nicobar Islands  2022        2   \n",
       "...                          ...   ...      ...   \n",
       "17563                   Nagaland  2018        4   \n",
       "17564                   Nagaland  2018        4   \n",
       "17565                   Nagaland  2018        4   \n",
       "17566                   Nagaland  2018        4   \n",
       "17567                   Nagaland  2018        4   \n",
       "\n",
       "                                district  registered_users  app_opens  \n",
       "0      north and middle andaman district             10720     904869  \n",
       "1                 south andaman district             63487    2368756  \n",
       "2                      nicobars district              2081     573782  \n",
       "3      north and middle andaman district             11379     963824  \n",
       "4                 south andaman district             66959    3404740  \n",
       "...                                  ...               ...        ...  \n",
       "17563                  tuensang district              1261          0  \n",
       "17564                     peren district               710          0  \n",
       "17565                   dimapur district             22603          0  \n",
       "17566                 zunheboto district              1099          0  \n",
       "17567                     wokha district              1391          0  \n",
       "\n",
       "[17568 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_user_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top_Insurance\n",
    "\n",
    "### Getting Data from /data/Top/Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_7 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/top/insurance/country/india/state/'\n",
    "top_ins_list = os.listdir(path_7)\n",
    "skl_data_7 = dict(state = [],year = [], quarter = [], pincode = [], transaction_count  = [],transaction_amount = [] )\n",
    "for state in top_ins_list:\n",
    "    curr_state = path_7 + state + '/' \n",
    "    year_list = os.listdir(curr_state)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_state + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file)\n",
    "            G = json.load(data)\n",
    "            \n",
    "            for item in G['data']['pincodes']:\n",
    "                pincode = item['entityName']\n",
    "                count = item['metric']['count']\n",
    "                amount = item['metric']['amount']\n",
    "                skl_data_7['state'].append(state)\n",
    "                skl_data_7['year'].append(year)\n",
    "                skl_data_7['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_7['pincode'].append(pincode)\n",
    "                skl_data_7['transaction_count'].append(count)\n",
    "                skl_data_7['transaction_amount'].append(amount)\n",
    "\n",
    "top_ins_df = pd.DataFrame(skl_data_7)\n",
    "top_ins_df['state'] = top_ins_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "top_ins_df['state'] = top_ins_df['state'].str.replace('-' ,' ') \n",
    "top_ins_df['state'] = top_ins_df['state'].str.title()\n",
    "top_ins_df['state'] = top_ins_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    " \n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top_Transaction\n",
    "\n",
    "### Getting Data from /data/Top/Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_8 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/top/transaction/country/india/state/'\n",
    "top_trans_list = os.listdir(path_8)\n",
    "skl_data_8 = dict(state = [],year = [], quarter = [], pincode = [], transaction_count  = [],transaction_amount = [] )\n",
    "for state in top_trans_list:\n",
    "    curr_state = path_8 + state + '/' \n",
    "    year_list = os.listdir(curr_state)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_state + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file)\n",
    "            H = json.load(data)\n",
    "            \n",
    "            for item in H['data']['pincodes']:\n",
    "                pincode = item['entityName']\n",
    "                count = item['metric']['count']\n",
    "                amount = item['metric']['amount']\n",
    "                skl_data_8['state'].append(state)\n",
    "                skl_data_8['year'].append(year)\n",
    "                skl_data_8['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_8['pincode'].append(pincode)\n",
    "                skl_data_8['transaction_count'].append(count)\n",
    "                skl_data_8['transaction_amount'].append(amount)\n",
    "\n",
    "top_trans_df = pd.DataFrame(skl_data_8)\n",
    "top_trans_df['state'] = top_trans_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "top_trans_df['state'] = top_trans_df['state'].str.replace('-' ,' ') \n",
    "top_trans_df['state'] = top_trans_df['state'].str.title()\n",
    "top_trans_df['state'] = top_trans_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    " \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top_User\n",
    "\n",
    "### Getting Data from /data/Top/User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_9 = '/Users/allen/Documents/Py VE/PhonePe Project/pulse/data/top/user/country/india/state/'\n",
    "top_user_list = os.listdir(path_9)\n",
    "skl_data_9 = dict(state = [],year = [], quarter = [], pincode = [], registered_users  = [])\n",
    "for state in top_user_list:\n",
    "    curr_state = path_9 + state + '/' \n",
    "    year_list = os.listdir(curr_state)\n",
    "    \n",
    "    for year in year_list:\n",
    "        curr_year = curr_state + year + '/'\n",
    "        file_list = os.listdir(curr_year)\n",
    "        \n",
    "        for file in file_list:\n",
    "            curr_file = curr_year + file\n",
    "            data = open(curr_file)\n",
    "            I = json.load(data)\n",
    "            \n",
    "            for item in I['data']['pincodes']:\n",
    "                pincode = item['name']\n",
    "                reg_user_count = item['registeredUsers']         \n",
    "                \n",
    "                skl_data_9['state'].append(state)\n",
    "                skl_data_9['year'].append(year)\n",
    "                skl_data_9['quarter'].append(int(file.strip('.json')))\n",
    "                skl_data_9['pincode'].append(pincode)\n",
    "                skl_data_9['registered_users'].append(reg_user_count)\n",
    "               \n",
    "\n",
    "top_user_df = pd.DataFrame(skl_data_9)\n",
    "top_user_df['state'] = top_user_df['state'].str.replace('andaman-&-nicobar-islands', 'Andaman & Nicobar Islands')\n",
    "top_user_df['state'] = top_user_df['state'].str.replace('-' ,' ') \n",
    "top_user_df['state'] = top_user_df['state'].str.title()\n",
    "top_user_df['state'] = top_user_df['state'].str.replace('Dadra & Nagar Haveli & Daman & Diu', 'Dadra and Nagar Haveli and Daman and Diu')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table Creation\n",
    "\n",
    "#SQL connection\n",
    "myDb = mysql.connector.connect(\n",
    "    host = 'localhost',\n",
    "    user = 'root',\n",
    "    password = 'Aa12345678!',\n",
    "    database = 'phonepe_visualization'\n",
    ")\n",
    "myCursor = myDb.cursor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "1050 (42S01): Table 'aggregrated_insurance' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMySQLInterfaceError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Py VE/.venv/lib/python3.9/site-packages/mysql/connector/connection_cext.py:661\u001b[0m, in \u001b[0;36mCMySQLConnection.cmd_query\u001b[0;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[1;32m    660\u001b[0m         query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 661\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mMySQLInterfaceError\u001b[0m: Table 'aggregrated_insurance' already exists",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Create Table Queries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m create_query_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mCREATE TABLE aggregrated_insurance(\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    state VARCHAR(255),\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    year INT,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    transaction_amount BIGINT\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    )\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmyCursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_query_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m myDb\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m     13\u001b[0m create_query_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mCREATE TABLE aggregrated_transaction(\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    state VARCHAR(255),\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    year INT,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m    transaction_amount BIGINT\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    )\u001b[39m\u001b[38;5;124m'''\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Py VE/.venv/lib/python3.9/site-packages/mysql/connector/cursor_cext.py:374\u001b[0m, in \u001b[0;36mCMySQLCursor.execute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot all parameters were used in the SQL statement\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcmd_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_mysql_exception(\n\u001b[1;32m    382\u001b[0m         msg\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mmsg, errno\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39merrno, sqlstate\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39msqlstate\n\u001b[1;32m    383\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Py VE/.venv/lib/python3.9/site-packages/mysql/connector/opentelemetry/context_propagation.py:74\u001b[0m, in \u001b[0;36mwith_context_propagation.<locals>.wrapper\u001b[0;34m(cnx, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Context propagation decorator.\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OTEL_ENABLED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cnx\u001b[38;5;241m.\u001b[39motel_context_propagation:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m current_span \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mget_current_span()\n\u001b[1;32m     77\u001b[0m tp_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Py VE/.venv/lib/python3.9/site-packages/mysql/connector/connection_cext.py:669\u001b[0m, in \u001b[0;36mCMySQLConnection.cmd_query\u001b[0;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmysql\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m    662\u001b[0m         query,\n\u001b[1;32m    663\u001b[0m         raw\u001b[38;5;241m=\u001b[39mraw,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m         query_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_attrs,\n\u001b[1;32m    667\u001b[0m     )\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_mysql_exception(\n\u001b[1;32m    670\u001b[0m         err\u001b[38;5;241m.\u001b[39merrno, msg\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mmsg, sqlstate\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39msqlstate\n\u001b[1;32m    671\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    673\u001b[0m     addr \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unix_socket \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unix_socket \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    675\u001b[0m     )\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 1050 (42S01): Table 'aggregrated_insurance' already exists"
     ]
    }
   ],
   "source": [
    "#CREATE TABLE Queries\n",
    "def create_tables():\n",
    "    create_query_1 = '''CREATE TABLE IF NOT EXIST aggregrated_insurance(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        transaction_type VARCHAR(255),\n",
    "        transaction_count BIGINT,\n",
    "        transaction_amount BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_1)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_2 = '''CREATE TABLE IF NOT EXIST aggregrated_transaction(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        transaction_type VARCHAR(255),\n",
    "        transaction_count BIGINT,\n",
    "        transaction_amount BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_2)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_3 = '''CREATE TABLE IF NOT EXIST aggregrated_user(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        brand VARCHAR(255),\n",
    "        transaction_count BIGINT,\n",
    "        percentage LONG\n",
    "        )'''\n",
    "    myCursor.execute(create_query_3)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_4 = '''CREATE TABLE IF NOT EXIST map_insurance(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        district VARCHAR(255),\n",
    "        transaction_count BIGINT,\n",
    "        transaction_amount BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_4)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_5 = '''CREATE TABLE IF NOT EXIST map_transaction(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        district VARCHAR(255),\n",
    "        transaction_count BIGINT,\n",
    "        transaction_amount BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_5)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_6 = '''CREATE TABLE IF NOT EXIST map_user(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        district VARCHAR(255),\n",
    "        registered_users BIGINT,\n",
    "        app_opens BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_6)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_7 = '''CREATE TABLE IF NOT EXIST top_insurance(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        pincode VARCHAR(255),\n",
    "        transaction_count INT,\n",
    "        transaction_amount BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_7)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_8 = '''CREATE TABLE IF NOT EXIST top_transaction(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        pincode VARCHAR(255),\n",
    "        transaction_count INT,\n",
    "        transaction_amount BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_8)\n",
    "    myDb.commit()\n",
    "\n",
    "    create_query_9 = '''CREATE TABLE IF NOT EXIST top_user(\n",
    "        state VARCHAR(255),\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        pincode VARCHAR(255),\n",
    "        registered_users BIGINT\n",
    "        )'''\n",
    "    myCursor.execute(create_query_9)\n",
    "    myDb.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_table_values():\n",
    "    insert_query_1 = '''INSERT INTO aggregrated_insurance\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = aggr_ins_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_1,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_2 = '''INSERT INTO aggregrated_transaction\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = aggr_trans_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_2,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_3 = '''INSERT INTO aggregrated_user\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = aggr_user_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_3,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_4 = '''INSERT INTO map_insurance\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = map_ins_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_4,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_5 = '''INSERT INTO map_transaction\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = map_trans_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_5,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_6 = '''INSERT INTO map_user\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = map_user_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_6,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_7 = '''INSERT INTO top_insurance\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = top_ins_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_7,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_8 = '''INSERT INTO top_transaction\n",
    "    VALUES (%s,%s,%s,%s,%s,%s)'''\n",
    "    values = top_trans_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_8,values)\n",
    "    myDb.commit()\n",
    "\n",
    "    insert_query_9 = '''INSERT INTO top_user\n",
    "    VALUES (%s,%s,%s,%s,%s)'''\n",
    "    values = top_user_df.values.tolist()\n",
    "    myCursor.executemany(insert_query_9,values)\n",
    "    myDb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aggr_user_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aggr_user_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aggr_user_df' is not defined"
     ]
    }
   ],
   "source": [
    "aggr_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                  object\n",
       "year                   object\n",
       "quarter                 int64\n",
       "transaction_type       object\n",
       "transaction_count       int64\n",
       "transaction_amount    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_tables()\n",
    "insert_table_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state', 'year', 'quarter', 'transaction_type', 'transaction_count',\n",
       "       'transaction_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr_ins_df.to_csv('agg_ins.csv',index=False)\n",
    "aggr_trans_df.to_csv('agg_trans.csv',index=False)\n",
    "aggr_user_df.to_csv('agg_user.csv',index=False)\n",
    "map_ins_df.to_csv('map_ins.csv',index=False)\n",
    "map_trans_df.to_csv('map_trans.csv',index=False)\n",
    "map_user_df.to_csv('map_user.csv',index=False)\n",
    "top_ins_df.to_csv('top_ins.csv',index=False)\n",
    "top_trans_df.to_csv('top_trans.csv',index=False)\n",
    "top_user_df.to_csv('top_user.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'aggr_user.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggr_user.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aggr_user.csv'"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv('/includes/aggr_user.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
